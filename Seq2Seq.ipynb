{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42069\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = '|<sos>|'\n",
    "eos = '|<eos>|'\n",
    "pad = '|<pad>|'\n",
    "oov = '|<oov>|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \n",
    "    def __init__(self, VocabList, language):\n",
    "        \n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        self.pad = pad\n",
    "        self.oov = oov\n",
    "        self.char2idx = {self.sos: 0, self.eos: 1, self.pad: 2, self.oov: 3}\n",
    "        self.idx2char = [self.sos, self.eos, self.pad, self.oov]\n",
    "        self.vocab_count = 4\n",
    "        \n",
    "        self.language = language\n",
    "        self.build_tokenizer()\n",
    "        self.build_vocab(VocabList)\n",
    "        \n",
    "        \n",
    "    def build_tokenizer(self):\n",
    "        if self.language == \"english\":\n",
    "            self.tokenizer = get_tokenizer('spacy', language='en')\n",
    "        elif self.language == \"spanish\":\n",
    "            self.tokenizer = get_tokenizer('spacy', language='es_core_news_lg')   \n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        return self.tokenizer(sentence)\n",
    "\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.char2idx:\n",
    "            self.char2idx[word] = self.vocab_count\n",
    "            self.idx2char.append(word)\n",
    "            self.vocab_count +=1 \n",
    "    \n",
    "    def build_vocab(self, vocabList):\n",
    "        for sentenceList in vocabList:\n",
    "            sentenceTokens = self.tokenize(sentenceList)\n",
    "            for token in sentenceTokens:\n",
    "                self.add_word(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        \n",
    "        \"\"\"\n",
    "        Source: English\n",
    "        Target: Spanish\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataframe = pd.read_csv(datapath)\n",
    "        self.english = self.dataframe['eng']\n",
    "        self.spanish = self.dataframe['spa']\n",
    "        \n",
    "        self.englishVocab = Vocab(self.english.to_list(), language=\"english\")\n",
    "        self.spanishVocab = Vocab(self.spanish.to_list(), language=\"spanish\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        eng = self.english[idx]\n",
    "        spa = self.spanish[idx]\n",
    "        \n",
    "        eng = self.englishVocab.tokenize(eng)\n",
    "        spa = self.spanishVocab.tokenize(spa) \n",
    "        #spa.reverse() ##Reversing word distance as stated in paper\n",
    "        \n",
    "        eng = [self.englishVocab.sos] + eng + [self.englishVocab.eos]\n",
    "        spa = [self.spanishVocab.sos] + spa + [self.spanishVocab.eos]\n",
    "        \n",
    "        eng = torch.LongTensor([self.englishVocab.char2idx[token] for token in eng])\n",
    "        \n",
    "        spa = torch.LongTensor([self.spanishVocab.char2idx[token] for token in spa])\n",
    "                \n",
    "        return  eng, spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = TranslationDataset('./data/eng-spa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(data):\n",
    "    eng = []\n",
    "    spa = []\n",
    "    \n",
    "    for dat in data:\n",
    "        eng.append(dat[0])\n",
    "        spa.append(dat[1])\n",
    "        \n",
    "    eng = pad_sequence(eng, padding_value = PAD_IDX, batch_first=True)\n",
    "    spa = pad_sequence(spa, padding_value = PAD_IDX, batch_first=True)\n",
    "    \n",
    "    return eng, spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, num_workers=18, batch_size=BATCH_SIZE, pin_memory=True, collate_fn=collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, hidden_dim, encoder_embedding_dim, num_layers):\n",
    "        \n",
    "        super(Encoder, self).__init__() \n",
    "        \n",
    "        self.vocab_size = vocab.vocab_count\n",
    "        self.embedding_dim = encoder_embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embedding_dim, hidden_dim, num_layers=self.num_layers, dropout=0.5, batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, sentence, encoder_hidden_state):\n",
    "        \n",
    "        embedded = self.emb(sentence)\n",
    "        \n",
    "        out, hidden_state = self.GRU(embedded, encoder_hidden_state)\n",
    "        \n",
    "        return out, hidden_state\n",
    "    \n",
    "    def initHidden(self, BATCH_SIZE):\n",
    "        return torch.zeros(self.num_layers, BATCH_SIZE ,self.hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, hidden_dim, embedding_dim, num_layers):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab.vocab_count\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.GRU = nn.GRU(self.embedding_dim, hidden_dim, num_layers=self.num_layers, batch_first=True, dropout=0.5)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_dim=self.embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, self.vocab_size)\n",
    "        \n",
    "    def forward(self, y, hidden_state):\n",
    "        \n",
    "        y = self.embedding(y)\n",
    "        \n",
    "        lstm_out, decoder_hidden_state = self.GRU(y, hidden_state)\n",
    "        \n",
    "        logits = self.fc(lstm_out)\n",
    "                \n",
    "        return logits, decoder_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding_dim, hidden_layers, english_vocab, spanish_vocab):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        self.eng_vocab = english_vocab\n",
    "        self.spa_vocab = spanish_vocab\n",
    "        \n",
    "        self.encoder = Encoder(self.eng_vocab, hidden_dim, embedding_dim, hidden_layers)\n",
    "        self.decoder = Decoder(self.spa_vocab, hidden_dim, embedding_dim, hidden_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, y, teacher_forcing = 0):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "        current_batch_size, max_seq_len = y.shape\n",
    "\n",
    "        encoder_hidden = self.encoder.initHidden(current_batch_size).to(device)\n",
    "        encoder_output, encoder_hidden = self.encoder.forward(x, encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        del encoder_hidden\n",
    "        \n",
    "        outputs = torch.zeros(size=(max_seq_len - 1, current_batch_size, self.decoder.vocab_size)).to(device)\n",
    "\n",
    "        prev_word = torch.zeros_like(y[:, 0])\n",
    "\n",
    "        for i in range(max_seq_len - 1):\n",
    "\n",
    "            if random.random() < teacher_forcing: #Teacher forcing\n",
    "                logits, decoder_hidden = self.decoder.forward(y[:, i].unsqueeze(1), decoder_hidden)\n",
    "            else:\n",
    "                logits, decoder_hidden = self.decoder.forward(prev_word.unsqueeze(1), decoder_hidden)  #Teacher forcing: Get random then pass i from y if > proba else pass previous scores: TODO\n",
    "\n",
    "            prev_word = logits.argmax(dim=-1).squeeze(1)\n",
    "\n",
    "            outputs[i] = logits.squeeze(1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(model, lr=1e-5, weight_decay=0):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(hidden_dim=1024, embedding_dim=1024, hidden_layers=16, english_vocab=dataset.englishVocab, spanish_vocab=dataset.spanishVocab).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = configure_optimizers(model)\n",
    "\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x, y, teacher_forcing = 0.5, clip=5):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with autocast():\n",
    "        outputs = model.forward(x, y, teacher_forcing)\n",
    "        loss = criterion(outputs.permute(1, 2, 0), y[:, 1:])\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    print(\"Epoch: {}, Started: {}\".format(epoch+1, time.ctime()))\n",
    "    print(\"---------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "    for batch_IDX, batch in enumerate(dataloader):    \n",
    "        \n",
    "        x, y = batch\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        batch_loss = train_step(model, optimizer, x, y)\n",
    "\n",
    "        epoch_loss += batch_loss\n",
    "\n",
    "        if batch_IDX % print_every == 0:\n",
    "            print(\"Epoch: {}, Batch: {},   Batch Loss: {:.4f} \".format(epoch+1, batch_IDX, batch_loss))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Epoch: {}, Mean Epoch Loss: {:.4f}\".format(epoch+1, epoch_loss / len(dataloader)))\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_translation(encoder, decoder, sentence, max_len=50):\n",
    "    \n",
    "    sentence = dataset.englishVocab.tokenize(sentence)\n",
    "    sentence = [[dataset.englishVocab.char2idx.get(token, dataset.englishVocab.char2idx[oov]) for token in sentence]]\n",
    "    sentence = torch.LongTensor(sentence).to(device)\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(1).to(device)\n",
    "    _, encoder_hidden = encoder(sentence, encoder_hidden)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    word = [[dataset.spanishVocab.char2idx[sos]]]\n",
    "    word = torch.LongTensor(word).to(device)\n",
    "    \n",
    "    translation = []\n",
    "    \n",
    "    i = 0\n",
    "    word_str = None\n",
    "    while i < max_len and word_str != eos:\n",
    "        \n",
    "        decoder_out, decoder_hidden = decoder(word, decoder_hidden)\n",
    "        \n",
    "        word = decoder_out.argmax(dim=-1)\n",
    "        print(word)\n",
    "        word_str = dataset.spanishVocab.idx2char[decoder_out.argmax().item()]\n",
    "        translation.append(word_str)\n",
    "        i += 1\n",
    "    \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    print(\" \".join(generate_translation(model.encoder, model.decoder, \"do you have something to say?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
